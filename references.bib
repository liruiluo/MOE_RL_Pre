@article{agarwal2021deep,
  title={Deep reinforcement learning at the edge of the statistical precipice},
  author={Agarwal, Rishabh and Schwarzer, Max and Castro, Pablo Samuel and Courville, Aaron C and Bellemare, Marc},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={29304--29320},
  year={2021}
}

@article{ash2020warm,
  title={On warm-starting neural network training},
  author={Ash, Jordan and Adams, Ryan P},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={3884--3894},
  year={2020}
}

@article{bellemare2013arcade,
  title={The arcade learning environment: An evaluation platform for general agents},
  author={Bellemare, Marc G and Naddaf, Yavar and Veness, Joel and Bowling, Michael},
  journal={Journal of Artificial Intelligence Research},
  volume={47},
  pages={253--279},
  year={2013}
}

@inproceedings{bellemare2017distributional,
  title={A distributional perspective on reinforcement learning},
  author={Bellemare, Marc G and Dabney, Will and Munos, R{\'e}mi},
  booktitle={International conference on machine learning},
  pages={449--458},
  year={2017}
}

@article{berner2019dota,
  title={Dota 2 with large scale deep reinforcement learning},
  author={Berner, Christopher and others},
  journal={arXiv preprint arXiv:1912.06680},
  year={2019}
}

@article{castro2018dopamine,
  title={Dopamine: A research framework for deep reinforcement learning},
  author={Castro, Pablo Samuel and Moitra, Subhodeep and Gelada, Carles and Kumar, Saurabh and Bellemare, Marc G},
  journal={arXiv preprint arXiv:1812.06110},
  year={2018}
}

@inproceedings{ceron2024mixtures,
  title={Mixtures of Experts Unlock Parameter Scaling for Deep RL},
  author={Ceron, Johan S Obando and Sokar, Ghada and Willi, Timon and Castro, Pablo Samuel and Evci, Utku},
  booktitle={International Conference on Machine Learning},
  year={2024}
}

@inproceedings{ceron2021revisiting,
  title={Revisiting rainbow: Promoting more insightful and inclusive deep reinforcement learning research},
  author={Ceron, Johan S Obando and Castro, Pablo Samuel},
  booktitle={International Conference on Machine Learning},
  pages={1373--1383},
  year={2021}
}

@inproceedings{dosovitskiy2021image,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey and others},
  booktitle={International Conference on Learning Representations},
  year={2021}
}

@inproceedings{doro2023sample,
  title={Sample-efficient reinforcement learning by breaking the replay ratio barrier},
  author={D'Oro, Pierluca and Schwarzer, Max and Nikishin, Evgenii and Bacon, Pierre-Luc and Bellemare, Marc G and Courville, Aaron},
  booktitle={International Conference on Learning Representations},
  year={2023}
}

@inproceedings{espeholt2018impala,
  title={Impala: Scalable distributed deep-rl with importance weighted actor-learner architectures},
  author={Espeholt, Lasse and others},
  booktitle={International conference on machine learning},
  pages={1407--1416},
  year={2018}
}

@inproceedings{evci2020rigging,
  title={Rigging the lottery: Making all tickets winners},
  author={Evci, Utku and Gale, Trevor and Menick, Jacob and Castro, Pablo Samuel and Elsen, Erich},
  booktitle={International Conference on Machine Learning},
  pages={2943--2952},
  year={2020}
}

@inproceedings{farebrother2022proto,
  title={Proto-value networks: Scaling representation learning with auxiliary tasks},
  author={Farebrother, Jesse and others},
  booktitle={International Conference on Learning Representations},
  year={2022}
}

@inproceedings{farebrother2024stop,
  title={Stop regressing: Training value functions via classification for scalable deep rl},
  author={Farebrother, Jesse and others},
  booktitle={International Conference on Machine Learning},
  year={2024}
}

@article{fedus2022switch,
  title={Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity},
  author={Fedus, William and Zoph, Barret and Shazeer, Noam},
  journal={The Journal of Machine Learning Research},
  volume={23},
  number={1},
  pages={5232--5270},
  year={2022}
}

@article{gale2019state,
  title={The state of sparsity in deep neural networks},
  author={Gale, Trevor and Elsen, Erich and Hooker, Sara},
  journal={arXiv preprint arXiv:1902.09574},
  year={2019}
}

@inproceedings{gale2023megablocks,
  title={MegaBlocks: Efficient Sparse Training with Mixture-of-Experts},
  author={Gale, Trevor and Narayanan, Deepak and Young, Cliff and Zaharia, Matei},
  booktitle={Proceedings of Machine Learning and Systems},
  volume={5},
  year={2023}
}

@inproceedings{graesser2022state,
  title={The state of sparse training in deep reinforcement learning},
  author={Graesser, Laura and Evci, Utku and Elsen, Erich and Castro, Pablo Samuel},
  booktitle={International Conference on Machine Learning},
  pages={7766--7792},
  year={2022}
}

@inproceedings{hessel2018rainbow,
  title={Rainbow: Combining improvements in deep reinforcement learning},
  author={Hessel, Matteo and others},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={32},
  year={2018}
}

@inproceedings{houlsby2019parameter,
  title={Parameter-efficient transfer learning for NLP},
  author={Houlsby, Neil and others},
  booktitle={International Conference on Machine Learning},
  pages={2790--2799},
  year={2019}
}

@inproceedings{kaiser2020model,
  title={Model based reinforcement learning for atari},
  author={Kaiser, Lukasz and others},
  booktitle={International Conference on Learning Representations},
  year={2020}
}

@article{kaplan2020scaling,
  title={Scaling laws for neural language models},
  author={Kaplan, Jared and others},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}

@article{kumar2020conservative,
  title={Conservative q-learning for offline reinforcement learning},
  author={Kumar, Aviral and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1179--1191},
  year={2020}
}

@inproceedings{kumar2021implicit,
  title={Implicit under-parameterization inhibits data-efficient deep reinforcement learning},
  author={Kumar, Aviral and others},
  booktitle={International Conference on Learning Representations},
  year={2021}
}

@inproceedings{kumar2021dr3,
  title={Dr3: Value-based deep reinforcement learning requires explicit regularization},
  author={Kumar, Aviral and others},
  booktitle={International Conference on Learning Representations},
  year={2021}
}

@inproceedings{kumar2022offline,
  title={Offline q-learning on diverse multi-task data both scales and generalizes},
  author={Kumar, Aviral and others},
  booktitle={International Conference on Learning Representations},
  year={2022}
}

@inproceedings{lyle2022understanding,
  title={Understanding and preventing capacity loss in reinforcement learning},
  author={Lyle, Clare and Rowland, Mark and Dabney, Will},
  booktitle={International Conference on Learning Representations},
  year={2022}
}

@inproceedings{lyle2022learning,
  title={Learning dynamics and generalization in deep reinforcement learning},
  author={Lyle, Clare and Rowland, Mark and Dabney, Will and Kwiatkowska, Marta and Gal, Yarin},
  booktitle={International Conference on Machine Learning},
  pages={14311--14335},
  year={2022}
}

@article{machado2018revisiting,
  title={Revisiting the arcade learning environment: evaluation protocols and open problems for general agents},
  author={Machado, Marlos C and others},
  journal={Journal of Artificial Intelligence Research},
  volume={61},
  pages={523--562},
  year={2018}
}

@article{mnih2015human,
  title={Human-level control through deep reinforcement learning},
  author={Mnih, Volodymyr and others},
  journal={nature},
  volume={518},
  number={7540},
  pages={529--533},
  year={2015}
}

@inproceedings{nikishin2022primacy,
  title={The primacy bias in deep reinforcement learning},
  author={Nikishin, Evgenii and others},
  booktitle={International Conference on Machine Learning},
  pages={16673--16706},
  year={2022}
}

@article{ostrovski2021difficulty,
  title={The difficulty of passive learning in deep reinforcement learning},
  author={Ostrovski, Georg and Castro, Pablo Samuel and Dabney, Will},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={28527--28540},
  year={2021}
}

@article{puigcerver2023from,
  title={From sparse to soft mixtures of experts},
  author={Puigcerver, Joan and others},
  journal={arXiv preprint arXiv:2308.00951},
  year={2023}
}

@inproceedings{ruiz2021scaling,
  title={Scaling vision with sparse mixture of experts},
  author={Ruiz, Carlos Riquelme and others},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={8583--8595},
  year={2021}
}

@inproceedings{schwarzer2023bigger,
  title={Bigger, better, faster: Human-level Atari with human-level efficiency},
  author={Schwarzer, Max and others},
  booktitle={International Conference on Machine Learning},
  pages={30365--30380},
  year={2023}
}

@inproceedings{shazeer2017outrageously,
  title={Outrageously large neural networks: The sparsely-gated mixture-of-experts layer},
  author={Shazeer, Noam and others},
  booktitle={International Conference on Learning Representations},
  year={2017}
}

@article{sokar2024tokenize,
  title={Don't flatten, tokenize! Unlocking the key to SoftMoE's efficacy in deep RL},
  author={Sokar, Ghada and Obando-Ceron, Johan S and Castro, Pablo Samuel and Evci, Utku},
  journal={arXiv preprint arXiv:2408.13637},
  year={2024}
}

@inproceedings{sokar2023dormant,
  title={The dormant neuron phenomenon in deep reinforcement learning},
  author={Sokar, Ghada and Agarwal, Rishabh and Castro, Pablo Samuel and Evci, Utku},
  booktitle={International Conference on Machine Learning},
  pages={32145--32168},
  year={2023}
}

@book{sutton1998introduction,
  title={Introduction to reinforcement learning},
  author={Sutton, Richard S and Barto, Andrew G},
  year={1998},
  publisher={MIT press}
}

@inproceedings{taiga2022investigating,
  title={Investigating multi-task pretraining and generalization in reinforcement learning},
  author={Taiga, Amine Ait and others},
  booktitle={International Conference on Learning Representations},
  year={2022}
}

@article{van2019parametric,
  title={When to use parametric models in reinforcement learning?},
  author={Van Hasselt, Hado P and Hessel, Matteo and Aslanides, John},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and others},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@inproceedings{willi2024mixture,
  title={Mixture of experts in a mixture of RL settings},
  author={Willi, Timon and Obando-Ceron, Johan S and Foerster, Jakob N and Dziugaite, Gintare Karolina and Castro, Pablo Samuel},
  booktitle={Reinforcement Learning Conference},
  year={2024}
}

@inproceedings{yarats2021image,
  title={Image augmentation is all you need: Regularizing deep reinforcement learning from pixels},
  author={Yarats, Denis and Kostrikov, Ilya and Fergus, Rob},
  booktitle={International Conference on Learning Representations},
  year={2021}
}

@inproceedings{zhou2022mixture,
  title={Mixture-of-experts with expert choice routing},
  author={Zhou, Yanqi and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={7103--7114},
  year={2022}
}

@article{berner2019dota2,
  title={Dota 2 with large scale deep reinforcement learning},
  author={Berner, Christopher and others},
  journal={arXiv preprint arXiv:1912.06680},
  year={2019}
}

@inproceedings{doro2023sample,
  title={Sample-efficient reinforcement learning by breaking the replay ratio barrier},
  author={D'Oro, Pierluca and Schwarzer, Max and Nikishin, Evgenii and Bacon, Pierre-Luc and Bellemare, Marc G and Courville, Aaron},
  booktitle={International Conference on Learning Representations},
  year={2023}
} 