\section{Related Works}
\label{sec:relatedWorks}
% This is consistent with the findings of \citet{kumar2023offline}, and

\paragraph{Mixture of Experts} Mixture of Experts (MoEs) have demonstrated remarkable success in scaling language and computer vision models to trillions of parameters \citep{lepikhin2020gshard, fedus2022switch, wang2020deep, yang2019condconv, abbas2020biased, pavlitskaya2020using}. %Despite these achievements, MoEs face challenges such as token balancing and training instabilities. To address these issues, expert choice routing \cite{zhou2022mixture} has been developed, enabling optimal load balancing by allowing each expert to select the top-$k$ tokens. SoftMoE \citep{puigcerver2024from} advances this approach by introducing fully-differentiable mixtures, where all input tokens are combined in various weighted combinations and passed to each expert, rather than routing tokens sparsely.  
\cite{chung2024beyond} examined the implicit biases arising from token combinations in SoftMoE and defined the notion of expert specialization in computer vision. Besides the success of MoEs in single task training, similar gains have been observed in transfer and multitask learning \citep{puigcerver2020scalable, chen2023mod, ye2023taskexpert,hazimeh2021dselect}. 
In addition to the works of \rebuttal{\citet{ceron2024mixtures} and \citet{willi2024mixture}} already discussed,
%Very recently few works have been explored MoEs in the field of RL. \cite{ceron2024mixtures} demonstrated that MoEs substantially improves agents performance and enable scalability of networks. \cite{willi2024mixture} showed similar improvements in multi-task learning. 
\cite{farebrother24classification} proposed replacing regression loss with classification loss and showed its benefits on MoEs. 


\paragraph{Network \rebuttal{scalability} in RL}
Scaling reinforcement learning (RL) networks presents significant challenges, largely due to issues with training instabilities \citep{hessel2018rainbow}. To address these difficulties, several techniques have been proposed, with a focus on algorithmic improvements or hyperparameter optimization \citep{farebrother2022proto,farebrother24classification,taiga2022investigating, schwarzer23bbf}. Recently, a few studies have begun to focus on architectural innovations as a means of addressing these challenges. \citet{kumar2023offline} showed that incorporating a learned spatial embedding with the output of a ResNet encoder, prior to flattening it, improves the scalability in offline RL. \cite{ceron2024pruned} demonstrated that gradual magnitude pruning enables the scaling of value-based agents, resulting in substantial performance gains. %Similarly, \cite{ceron2024mixtures} showed that Mixture of Experts (MoEs), particularly SoftMoE, significantly enhance the performance of scaled networks. In this work, we focus on understanding the underlying causes of these improvements in MoEs.

% \paragraph{Sparsity in RL} 
% A related line of research investigates the application of sparse neural networks for reinforcement learning (RL) agents in both online \citep{ceron2024pruned,xu2024novel,graesser2022state,tanrlx2,sokar2022dynamic} and offline \citep{arnob2021single} settings. These studies demonstrate that it is possible to prune up to 90\% of the network parameters without negatively affecting the performance. In addition, \cite{sokar2023dormant} recently showed that throughout training, the number of dormant (inactive) neurons is increasing. These findings are aligned with our observation that not all features (or tokens) are equally significant.

\paragraph{Network \rebuttal{plasticity} in RL} It has been observed that networks trained on non-stationary distributions tend to lose their expressivity over time, a phenomenon referred to as the loss of plasticity \citep{kumar2020implicit, lyle2021understanding}. While several studies have attempted to explain this phenomenon \citep{lyle2023understanding, ma2023revisiting, lyle2024disentangling}, the underlying causes remain uncertain. In a parallel line of research, numerous methods have been proposed to improve network trainability. These approaches involve using small batch size for gradient updates \citep{ceron2023small}, periodically resetting some of the network's parameters to random values \citep{sokar2023dormant, nikishin2022primacy,igl2020transient,dohare2024loss}, slightly modifying the existing weights by shrinking and perturbing them \citep{d'oro2022sampleefficient,schwarzer23bbf}, and restricting the large deviation from initial weights \citep{kumar2023maintaining,lewandowski2024d}. Recently, \rebuttal{\citet{leeslow}} have also demonstrated that combining reinitialization with a teacher-student framework can enhance plasticity. Another technique involves preserving useful neurons by perturbing gradients to ensure smaller updates for important neurons \rebuttal{\citep{elsayedaddressing}}. In this work, we explore some of these methods to improve plasticity and utilization of experts.


